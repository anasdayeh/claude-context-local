"""Code embedding wrapper using EmbeddingGemma model."""

import logging
import os
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
import numpy as np

from chunking.code_chunk import CodeChunk
from embeddings.embedding_models_register import AVAILIABLE_MODELS
from common_utils import get_storage_dir


@dataclass
class EmbeddingResult:
    """Result of embedding generation."""
    embedding: np.ndarray
    chunk_id: str
    metadata: Dict[str, Any]


class CodeEmbedder:
    """Wrapper for embedding code chunks using EmbeddingGemma model."""

    def __init__(
        self,
        model_name: str = "google/embeddinggemma-300m",
        cache_dir: Optional[str] = None,
        device: str = "auto"
    ):
        """Initialize code embedder.

        Args:
            model_name: Name of the embedding model to use
            cache_dir: Directory to cache the model
            device: Device to load model on
        """
        if device == "auto":
            env_device = os.getenv("CODE_SEARCH_DEVICE")
            if env_device:
                device = env_device

        if not cache_dir: # if not provided, use default
            cache_dir = str(get_storage_dir() / "models")
        self.device = device
        self._document_prompt_name: Optional[str] = None
        self._query_prompt_name: Optional[str] = None

        # Get model class from available models
        model_class = AVAILIABLE_MODELS[model_name]
        self._model = model_class(cache_dir=cache_dir, device=device)

        self._logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)

    @property
    def model(self):
        """Get the underlying embedding model."""
        return self._model.model

    def _resolve_prompt_name(self, candidates: List[str]) -> Optional[str]:
        """Pick the first available prompt name from the model registry."""
        prompts = getattr(self.model, "prompts", None)
        if not isinstance(prompts, dict):
            return None
        for name in candidates:
            if name in prompts:
                return name
        return None

    def _get_document_prompt_name(self) -> Optional[str]:
        if self._document_prompt_name is None:
            self._document_prompt_name = self._resolve_prompt_name(
                ["Retrieval-document", "document", "text"]
            )
            if not self._document_prompt_name:
                self._logger.warning(
                    "No document prompt found in model registry; embedding without a prompt."
                )
        return self._document_prompt_name

    def _get_query_prompt_name(self) -> Optional[str]:
        if self._query_prompt_name is None:
            self._query_prompt_name = self._resolve_prompt_name(
                ["InstructionRetrieval", "query"]
            )
            if not self._query_prompt_name:
                self._logger.warning(
                    "No query prompt found in model registry; embedding without a prompt."
                )
        return self._query_prompt_name

    def _encode_documents(self, texts: List[str]) -> np.ndarray:
        """Encode documents with the best available prompt."""
        prompt_name = self._get_document_prompt_name()
        encode_kwargs = {"show_progress_bar": False}
        if prompt_name:
            encode_kwargs["prompt_name"] = prompt_name

        model = self.model
        if hasattr(model, "encode_document"):
            embeddings = model.encode_document(texts, **encode_kwargs)
        else:
            embeddings = model.encode(texts, **encode_kwargs)
        return np.asarray(embeddings, dtype=np.float32)

    def _encode_queries(self, texts: List[str]) -> np.ndarray:
        """Encode queries with the best available prompt."""
        prompt_name = self._get_query_prompt_name()
        encode_kwargs = {"show_progress_bar": False}
        if prompt_name:
            encode_kwargs["prompt_name"] = prompt_name

        model = self.model
        if hasattr(model, "encode_query"):
            embeddings = model.encode_query(texts, **encode_kwargs)
        else:
            embeddings = model.encode(texts, **encode_kwargs)
        return np.asarray(embeddings, dtype=np.float32)

    def create_embedding_content(self, chunk: CodeChunk, max_chars: int = 6000) -> str:
        """Create clean content for embedding generation.

        Args:
            chunk: Code chunk to create content for
            max_chars: Maximum characters to include

        Returns:
            Content string for embedding
        """
        content_parts = []

        # Add docstring if available
        docstring_budget = 300
        if chunk.docstring:
            docstring = chunk.docstring[:docstring_budget] + "..." if len(chunk.docstring) > docstring_budget else chunk.docstring
            content_parts.append(f'"""{docstring}"""')

        # Calculate remaining budget for code content
        docstring_len = len(content_parts[0]) if content_parts else 0
        remaining_budget = max_chars - docstring_len - 10

        # Add code content with smart truncation
        if len(chunk.content) <= remaining_budget:
            content_parts.append(chunk.content)
        else:
            lines = chunk.content.split('\n')
            if len(lines) > 3:
                head_lines = []
                tail_lines = []
                current_length = docstring_len

                # Add head lines
                for line in lines[:min(len(lines)//2, 20)]:
                    if current_length + len(line) + 1 > remaining_budget * 0.7:
                        break
                    head_lines.append(line)
                    current_length += len(line) + 1

                # Add tail lines
                remaining_space = remaining_budget - current_length - 20
                for line in reversed(lines[-min(len(lines)//3, 10):]):
                    if len('\n'.join(tail_lines)) + len(line) + 1 > remaining_space:
                        break
                    tail_lines.insert(0, line)

                if tail_lines:
                    truncated_content = '\n'.join(head_lines) + '\n    # ... (truncated) ...\n' + '\n'.join(tail_lines)
                else:
                    truncated_content = '\n'.join(head_lines) + '\n    # ... (truncated) ...'
                content_parts.append(truncated_content)
            else:
                content_parts.append(chunk.content[:remaining_budget] + "..." if len(chunk.content) > remaining_budget else chunk.content)

        return '\n'.join(content_parts)

    def embed_chunk(self, chunk: CodeChunk) -> EmbeddingResult:
        """Generate embedding for a single code chunk.

        Args:
            chunk: Code chunk to embed

        Returns:
            EmbeddingResult with embedding and metadata
        """
        content = self.create_embedding_content(chunk)

        embedding = self._encode_documents([content])[0]

        # Create chunk ID
        chunk_id = f"{chunk.relative_path}:{chunk.start_line}-{chunk.end_line}:{chunk.chunk_type}"
        if chunk.name:
            chunk_id += f":{chunk.name}"

        # Prepare metadata
        metadata = {
            'file_path': chunk.file_path,
            'relative_path': chunk.relative_path,
            'folder_structure': chunk.folder_structure,
            'chunk_type': chunk.chunk_type,
            'start_line': chunk.start_line,
            'end_line': chunk.end_line,
            'name': chunk.name,
            'parent_name': chunk.parent_name,
            'docstring': chunk.docstring,
            'decorators': chunk.decorators,
            'imports': chunk.imports,
            'complexity_score': chunk.complexity_score,
            'tags': chunk.tags,
            'content_preview': chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content
        }

        return EmbeddingResult(
            embedding=embedding,
            chunk_id=chunk_id,
            metadata=metadata
        )

    def embed_chunks(self, chunks: List[CodeChunk], batch_size: int = 32) -> List[EmbeddingResult]:
        """Generate embeddings for multiple chunks with batching.

        Args:
            chunks: List of code chunks to embed
            batch_size: Batch size for processing

        Returns:
            List of EmbeddingResults
        """
        results = []

        env_batch = os.getenv("CODE_SEARCH_BATCH_SIZE")
        if env_batch:
            try:
                batch_size = max(1, int(env_batch))
            except ValueError:
                pass

        self._logger.info(f"Generating embeddings for {len(chunks)} chunks")

        # Process in batches
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_contents = [self.create_embedding_content(chunk) for chunk in batch]

            batch_embeddings = self._encode_documents(batch_contents)

            # Create results
            for chunk, embedding in zip(batch, batch_embeddings):
                chunk_id = f"{chunk.relative_path}:{chunk.start_line}-{chunk.end_line}:{chunk.chunk_type}"
                if chunk.name:
                    chunk_id += f":{chunk.name}"

                metadata = {
                    'file_path': chunk.file_path,
                    'relative_path': chunk.relative_path,
                    'folder_structure': chunk.folder_structure,
                    'chunk_type': chunk.chunk_type,
                    'start_line': chunk.start_line,
                    'end_line': chunk.end_line,
                    'name': chunk.name,
                    'parent_name': chunk.parent_name,
                    'docstring': chunk.docstring,
                    'decorators': chunk.decorators,
                    'imports': chunk.imports,
                    'complexity_score': chunk.complexity_score,
                    'tags': chunk.tags,
                    'content_preview': chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content
                }

                results.append(EmbeddingResult(
                    embedding=embedding,
                    chunk_id=chunk_id,
                    metadata=metadata
                ))

            if i + batch_size < len(chunks):
                self._logger.info(f"Processed {i + batch_size}/{len(chunks)} chunks")

        self._logger.info("Embedding generation completed")
        return results

    def embed_query(self, query: str) -> np.ndarray:
        """Generate embedding for a search query.

        Args:
            query: Search query text

        Returns:
            Embedding vector
        """
        return self._encode_queries([query])[0]

    def embed_document(self, text: str) -> np.ndarray:
        """Generate a document embedding for arbitrary text (same prompt path as chunks)."""
        return self._encode_documents([text])[0]

    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the embedding model.

        Returns:
            Dictionary with model information
        """
        return self._model.get_model_info()

    def cleanup(self):
        """Clean up model resources."""
        self._model.cleanup()

    def __del__(self):
        """Ensure cleanup on object destruction."""
        try:
            self.cleanup()
        except Exception:
            pass
